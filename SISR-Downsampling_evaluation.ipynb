{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b4fcecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bda17653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "Directories = ['Set5', 'Set14', 'Urban100', 'BSD100', 'Celeba_HQ']\n",
    "scales = [2,3,4,8]\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img = Image.open(folder + \"/\" + filename)\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c86dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "from PIL import Image \n",
    "import PIL \n",
    "\n",
    "def downsample(): \n",
    "    for directory in Directories:\n",
    "        images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR')\n",
    "        for scale in scales: \n",
    "            disk_dir = '/Users/ege/Desktop/DataSets/' + directory + \"/LR/\" + str(scale) + \"_b/\"\n",
    "            for i, oriimg in enumerate(images): \n",
    "                width, height = oriimg.size\n",
    "                #img = oriimg.resize((int(width/scale),int(height/scale)), Image.BICUBIC)\n",
    "                #img = cv2.resize(oriimg,None, fx=1/scale, fy=1/scale, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "                img.save(disk_dir +str(i)+ \".png\",\"PNG\")\n",
    "                print(img.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "73836abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sewar.full_ref import mse, rmse, psnr, rmse_sw, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb \n",
    "\n",
    "def evaluate(img1,img2):\n",
    "    print(ssim(img1,img2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b627682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample\n",
    "# Load Images \n",
    "\n",
    "Directories = ['Set14']\n",
    "scales = [2,3,4,8]\n",
    "\n",
    "def upsample():\n",
    "    for directory in Directories:\n",
    "        for scale in scales: \n",
    "            images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/LR/\" + str(scale)) \n",
    "            disk_dir = '/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\"\n",
    "            for i, oriimg in enumerate(images): \n",
    "                width, height = oriimg.size\n",
    "                img = oriimg.resize((int(width*scale),int(height*scale)), Image.BICUBIC)\n",
    "                #img = cv2.resize(oriimg,None, fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "                img.save(disk_dir +str(i)+ \".png\",\"PNG\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "71c8939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr_numpy(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5b047956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder:4_upscaled image number:0\n",
      "Folder:4_upscaled image number:2\n",
      "Folder:4_upscaled image number:12\n",
      "(14, 13)\n",
      "            0\n",
      "0   31.722074\n",
      "1   33.022681\n",
      "2   33.022681\n",
      "3   30.954153\n",
      "4   30.823561\n",
      "5   33.916186\n",
      "6   29.605766\n",
      "7   29.950692\n",
      "8   32.558987\n",
      "9   33.130295\n",
      "10  31.692397\n",
      "11  29.036801\n",
      "12  29.036801\n",
      "13  32.482327\n",
      "          0        0      0  0   0           0          0          0  \\\n",
      "0   bicubic  bicubic  Set14  4   0         NaN        NaN        NaN   \n",
      "1   bicubic  bicubic  Set14  4   1  587.927949  24.247226  20.437563   \n",
      "2   bicubic  bicubic  Set14  4   2         NaN        NaN        NaN   \n",
      "3   bicubic  bicubic  Set14  4   3  285.003467  16.882046  23.582302   \n",
      "4   bicubic  bicubic  Set14  4   4  235.425629  15.343586  24.412266   \n",
      "5   bicubic  bicubic  Set14  4   5  154.345134  12.423572  26.245874   \n",
      "6   bicubic  bicubic  Set14  4   6  321.561325  17.932131  23.058166   \n",
      "7   bicubic  bicubic  Set14  4   7  259.378577  16.105234  23.991463   \n",
      "8   bicubic  bicubic  Set14  4   8   95.067094   9.750236  28.350501   \n",
      "9   bicubic  bicubic  Set14  4   9  166.562507  12.905910  25.915031   \n",
      "10  bicubic  bicubic  Set14  4  10   83.225491   9.122801  28.928240   \n",
      "11  bicubic  bicubic  Set14  4  11  609.689281  24.691887  20.279718   \n",
      "12  bicubic  bicubic  Set14  4  12         NaN        NaN        NaN   \n",
      "13  bicubic  bicubic  Set14  4  13  124.454281  11.155908  27.180705   \n",
      "\n",
      "            0          0         0                   0         0  \n",
      "0   31.722074        NaN       NaN N00000000a00000000N       NaN  \n",
      "1   33.022681  19.848709  0.116170  0.947340+0.000000j  0.116170  \n",
      "2   33.022681        NaN       NaN N00000000a00000000N       NaN  \n",
      "3   30.954153  23.630777  0.141863  0.910320+0.000000j  0.141863  \n",
      "4   30.823561  24.394289  0.145022  0.922387+0.000000j  0.145022  \n",
      "5   33.916186  26.539516  0.115668  0.969623+0.000000j  0.115668  \n",
      "6   29.605766  23.032723  0.142445  0.888437+0.000000j  0.142445  \n",
      "7   29.950692  24.009192  0.124143  0.835552+0.000000j  0.124143  \n",
      "8   32.558987  28.503951  0.078977  0.951180+0.000000j  0.078977  \n",
      "9   33.130295  26.168310  0.073410  0.970256+0.000000j  0.073410  \n",
      "10  31.692397  29.067270  0.092199  0.944347+0.000000j  0.092199  \n",
      "11  29.036801  19.993368  0.213187  0.839583+0.000000j  0.213187  \n",
      "12  29.036801        NaN       NaN N00000000a00000000N       NaN  \n",
      "13  32.482327  28.194903  0.094729  0.964371+0.000000j  0.094729  \n"
     ]
    }
   ],
   "source": [
    "# Read and evaluate\n",
    "Directories = ['Set14']\n",
    "scales = [4]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sewar.full_ref import mse, rmse, rmse_sw, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb, psnr\n",
    "\n",
    "metric_name = []\n",
    "upsampling_method = []\n",
    "downsampling_method = []\n",
    "image_set = []\n",
    "image_scale = [] \n",
    "image_number = []\n",
    "metric_1 = []\n",
    "metric_2 = []\n",
    "metric_3 = []\n",
    "metric_4 = []\n",
    "metric_5 = []\n",
    "metric_6 = []\n",
    "metric_7 = []\n",
    "metric_8 = []\n",
    "\n",
    "for directory in Directories:    \n",
    "    for scale in scales: \n",
    "        images_1 = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + \"Original\") \n",
    "        images_2 =  load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\")\n",
    "\n",
    "        #images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR')\n",
    "        for i, oriimg in enumerate(images_1): \n",
    "            #evaluated_value = ssim(oriimg,images_2[i]) #evaluate(oriimg,images_2[i])\n",
    "            #value_mse = uqi(images_1[i],images_2[i])\n",
    "            #print(value_mse)\n",
    "            \n",
    "            pil_image_1 = images_1[i].convert('RGB') \n",
    "            pil_image_2 = images_2[i].convert('RGB')\n",
    "            \n",
    "            open_cv_image_1 = np.array(pil_image_1) \n",
    "            open_cv_image_2 = np.array(pil_image_2) \n",
    "            \n",
    "            # Convert RGB to BGR \n",
    "            open_cv_image_1 = open_cv_image_1[:, :, ::-1].copy()\n",
    "            open_cv_image_2 = open_cv_image_2[:, :, ::-1].copy()\n",
    "            \n",
    "            try:\n",
    "                mse_value = mse(open_cv_image_1,open_cv_image_2) #evaluate(oriimg,images_2[i])\n",
    "                rmse_value = rmse(open_cv_image_1,open_cv_image_2)\n",
    "                psnr_value = psnr(open_cv_image_1,open_cv_image_2)\n",
    "                rmse_sw_value = psnr_numpy(open_cv_image_1,open_cv_image_2)\n",
    "                ssim_value = ssim(open_cv_image_1,open_cv_image_2)\n",
    "                sam_value = sam(open_cv_image_1,open_cv_image_2)\n",
    "                msssim_value = msssim(open_cv_image_1,open_cv_image_2)\n",
    "                psnrb_value = psnrb(open_cv_image_1,open_cv_image_2) \n",
    "                \n",
    "            except:\n",
    "                mse_value = None\n",
    "                rmse_value = None\n",
    "                psnr_value = None\n",
    "                ssim_value = None\n",
    "                sam_value = None\n",
    "                msssim_value = None\n",
    "                psnrb_value = None\n",
    "                \n",
    "                print('Folder:' + str(scale) + '_upscaled image number:' + str(i))\n",
    "                pass\n",
    "            \n",
    "\n",
    "            # Merge all the lists/Pandas Dataframes in one and then concatenate them as a LIST\n",
    "            upsampling_method.append('bicubic')\n",
    "            downsampling_method.append('bicubic')\n",
    "            image_set.append(str(directory))\n",
    "            image_scale.append(str(scale))\n",
    "            image_number.append(str(i))\n",
    "            metric_1.append(mse_value)\n",
    "            metric_2.append(rmse_value)\n",
    "            metric_3.append(psnr_value)\n",
    "            metric_4.append(rmse_sw_value)\n",
    "            metric_5.append(psnrb_value)\n",
    "            metric_6.append(sam_value)\n",
    "            metric_7.append(msssim_value)\n",
    "            metric_8.append(sam_value)\n",
    "            \n",
    "\n",
    "            \n",
    "d_1 =  pd.DataFrame(metric_1)\n",
    "d_2 =  pd.DataFrame(metric_2)\n",
    "d_3 =  pd.DataFrame(metric_3)\n",
    "d_4 =  pd.DataFrame(metric_4)\n",
    "d_5 =  pd.DataFrame(metric_5)\n",
    "d_6 =  pd.DataFrame(metric_6)\n",
    "d_7 =  pd.DataFrame(metric_7)\n",
    "d_8 =  pd.DataFrame(metric_8)\n",
    "m_1 =  pd.DataFrame(upsampling_method)\n",
    "m_2 =  pd.DataFrame(downsampling_method)\n",
    "i_s =  pd.DataFrame(image_set)\n",
    "i_m =  pd.DataFrame(image_scale)\n",
    "i_n =  pd.DataFrame(image_number)\n",
    "\n",
    "\n",
    "        \n",
    "frames = [m_1,m_2,i_s,i_m,i_n,d_1,d_2,d_3,d_4,d_5,d_6,d_7,d_8]\n",
    "result = pd.concat(frames, axis=1)\n",
    "\n",
    "            # Save the big LIST again as Dataframe and then save it as list_2.csv\n",
    "df = pd.DataFrame(result)\n",
    "print(df.shape)\n",
    "print(d_4)\n",
    "print(df)\n",
    "df.to_csv('first_evaluation_test_2.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4308b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    31.496814\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(d_4.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d263f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.761984\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(d_3.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "36f33f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        0      0  0   0          0  0\n",
      "0   bicubic  bicubic  Set14  2   0  30.684806  *\n",
      "1   bicubic  bicubic  Set14  2   1  26.585917  *\n",
      "2   bicubic  bicubic  Set14  2   2  26.016277  *\n",
      "3   bicubic  bicubic  Set14  2   3  27.942775  *\n",
      "4   bicubic  bicubic  Set14  2   4  29.265910  *\n",
      "5   bicubic  bicubic  Set14  2   5  32.931336  *\n",
      "6   bicubic  bicubic  Set14  2   6  27.815216  *\n",
      "7   bicubic  bicubic  Set14  2   7  29.133704  *\n",
      "8   bicubic  bicubic  Set14  2   8  34.145069  *\n",
      "9   bicubic  bicubic  Set14  2   9  32.717302  *\n",
      "10  bicubic  bicubic  Set14  2  10  35.002439  *\n",
      "11  bicubic  bicubic  Set14  2  11  24.530641  *\n",
      "12  bicubic  bicubic  Set14  2  12  30.571258  *\n",
      "13  bicubic  bicubic  Set14  2  13  33.257687  *\n",
      "          0        0       0  0   0          0  0\n",
      "0   bicubic  bicubic  BSD100  2   0  32.314055  *\n",
      "1   bicubic  bicubic  BSD100  2   1  29.796060  *\n",
      "2   bicubic  bicubic  BSD100  2   2  28.050072  *\n",
      "3   bicubic  bicubic  BSD100  2   3  23.181740  *\n",
      "4   bicubic  bicubic  BSD100  2   4  34.067370  *\n",
      "..      ...      ...     ... ..  ..        ... ..\n",
      "95  bicubic  bicubic  BSD100  2  95  31.006065  *\n",
      "96  bicubic  bicubic  BSD100  2  96  28.304783  *\n",
      "97  bicubic  bicubic  BSD100  2  97  34.709624  *\n",
      "98  bicubic  bicubic  BSD100  2  98  28.811232  *\n",
      "99  bicubic  bicubic  BSD100  2  99  27.648693  *\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "         0        0     0  0  0          0  0\n",
      "0  bicubic  bicubic  Set5  2  0  37.119882  *\n",
      "1  bicubic  bicubic  Set5  2  1  32.164316  *\n",
      "2  bicubic  bicubic  Set5  2  2  36.861960  *\n",
      "3  bicubic  bicubic  Set5  2  3  27.424867  *\n",
      "4  bicubic  bicubic  Set5  2  4  35.027003  *\n",
      "          0        0         0  0   0          0  0\n",
      "0   bicubic  bicubic  Urban100  2   0  35.595495  *\n",
      "1   bicubic  bicubic  Urban100  2   1  27.945944  *\n",
      "2   bicubic  bicubic  Urban100  2   2  19.958492  *\n",
      "3   bicubic  bicubic  Urban100  2   3  28.855434  *\n",
      "4   bicubic  bicubic  Urban100  2   4  27.825107  *\n",
      "..      ...      ...       ... ..  ..        ... ..\n",
      "95  bicubic  bicubic  Urban100  2  95  27.075162  *\n",
      "96  bicubic  bicubic  Urban100  2  96  26.124789  *\n",
      "97  bicubic  bicubic  Urban100  2  97  29.208366  *\n",
      "98  bicubic  bicubic  Urban100  2  98  23.845748  *\n",
      "99  bicubic  bicubic  Urban100  2  99  21.413571  *\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "          0        0          0  0   0          0  0\n",
      "0   bicubic  bicubic  Celeba_HQ  2   0  48.048917  *\n",
      "1   bicubic  bicubic  Celeba_HQ  2   1  44.311688  *\n",
      "2   bicubic  bicubic  Celeba_HQ  2   2  47.644574  *\n",
      "3   bicubic  bicubic  Celeba_HQ  2   3  47.030544  *\n",
      "4   bicubic  bicubic  Celeba_HQ  2   4  45.828012  *\n",
      "..      ...      ...        ... ..  ..        ... ..\n",
      "95  bicubic  bicubic  Celeba_HQ  2  95  49.232361  *\n",
      "96  bicubic  bicubic  Celeba_HQ  2  96  39.845527  *\n",
      "97  bicubic  bicubic  Celeba_HQ  2  97  42.570523  *\n",
      "98  bicubic  bicubic  Celeba_HQ  2  98  37.878969  *\n",
      "99  bicubic  bicubic  Celeba_HQ  2  99  43.443410  *\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "           0          0  0\n",
      "0      Set14  30.042881  2\n",
      "1     BSD100  29.566708  2\n",
      "2       Set5  33.719606  2\n",
      "3   Urban100  26.595256  2\n",
      "4  Celeba_HQ  44.543068  2\n"
     ]
    }
   ],
   "source": [
    "# Read and evaluate\n",
    "Directories = ['Set14','BSD100','Set5','Urban100','Celeba_HQ']\n",
    "scales = [2]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.color import rgb2ycbcr\n",
    "from skimage import img_as_float\n",
    "\n",
    "from sewar.full_ref import mse, rmse, rmse_sw, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb, psnr\n",
    "\n",
    "Image_DataSet_List = []\n",
    "Image_Scaled_List = []\n",
    "Average_Metric_0_List = []\n",
    "Average_Metric_1_List = []\n",
    "Scale_List = []\n",
    "\n",
    "\n",
    "for directory in Directories:  \n",
    "    metric_name = []\n",
    "    upsampling_method = []\n",
    "    downsampling_method = []\n",
    "    image_set = []\n",
    "    image_scale = [] \n",
    "    image_number = []\n",
    "    metric_1 = []\n",
    "    metric_2 = []\n",
    "    for scale in scales: \n",
    "        images_1 = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + \"Original\") \n",
    "        images_2 =  load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\")\n",
    "        Scale_List.append(scale)   \n",
    "        #images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR')\n",
    "        for i, oriimg in enumerate(images_1): \n",
    "            \n",
    "            pil_image_1 = images_1[i].convert('RGB') \n",
    "            pil_image_2 = images_2[i].convert('RGB')\n",
    "            \n",
    "            open_cv_image_1 = np.array(pil_image_1) \n",
    "            open_cv_image_2 = np.array(pil_image_2) \n",
    "            \n",
    "            # Convert RGB to BGR \n",
    "            open_cv_image_1 = open_cv_image_1[:, :, ::-1].copy()\n",
    "            open_cv_image_2 = open_cv_image_2[:, :, ::-1].copy()\n",
    "            \n",
    "            im1_t = np.atleast_3d(img_as_float(open_cv_image_1))\n",
    "            im2_t = np.atleast_3d(img_as_float(open_cv_image_2))\n",
    "\n",
    "            if im1_t.shape[2] == 1 or im2_t.shape[2] == 1:\n",
    "                im1_t = im1_t[..., 0]\n",
    "                im2_t = im2_t[..., 0]\n",
    "\n",
    "            else:\n",
    "                im1_t = rgb2ycbcr(im1_t)[:, :, 0:1] / 255.0\n",
    "                im2_t = rgb2ycbcr(im2_t)[:, :, 0:1] / 255.0\n",
    "\n",
    "            try: \n",
    "                psnr_val = peak_signal_noise_ratio(im1_t, im2_t)\n",
    "            except:\n",
    "                print('error at: ' + str(i))\n",
    "\n",
    "            # Merge all the lists/Pandas Dataframes in one and then concatenate them as a LIST\n",
    "            upsampling_method.append('bicubic')\n",
    "            downsampling_method.append('bicubic')\n",
    "            image_set.append(str(directory))\n",
    "            image_scale.append(str(scale))\n",
    "            image_number.append(str(i))\n",
    "            metric_1.append(psnr_val)\n",
    "            metric_2.append('*')\n",
    "          \n",
    "        \n",
    "        \n",
    "    d_1 =  pd.DataFrame(metric_1)\n",
    "    d_2 =  pd.DataFrame(metric_2)\n",
    "    m_1 =  pd.DataFrame(upsampling_method)\n",
    "    m_2 =  pd.DataFrame(downsampling_method)\n",
    "    i_s =  pd.DataFrame(image_set)\n",
    "    i_m =  pd.DataFrame(image_scale)\n",
    "    i_n =  pd.DataFrame(image_number)\n",
    "\n",
    "    frames = [m_1,m_2,i_s,i_m,i_n,d_1,d_2]\n",
    "    result = pd.concat(frames, axis=1)\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    Image_DataSet_List.append(directory)\n",
    "    Average_Metric_0_List.append(d_1.mean())\n",
    "    Average_Metric_1_List.append(d_2.mean())\n",
    "   \n",
    "    \n",
    "\n",
    "I_D_L =  pd.DataFrame(Image_DataSet_List)\n",
    "A_M_0 =  pd.DataFrame(Average_Metric_0_List)\n",
    "A_M_1 =  pd.DataFrame(Average_Metric_1_List)\n",
    "S_L   =  pd.DataFrame(Scale_List)\n",
    "    \n",
    "frames_overall = [I_D_L,A_M_0,A_M_1,S_L]\n",
    "result_overall = pd.concat(frames_overall, axis=1)\n",
    "df_overall= pd.DataFrame(result_overall)\n",
    "\n",
    "print(df_overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "846088dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26.014496\n",
       "dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
