{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4fcecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bda17653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "Directories = ['Set5', 'Set14', 'Urban100', 'BSD100', 'Celeba_HQ']\n",
    "scales = [2,4,8]\n",
    "import re\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    os_list_range = len(os.listdir(folder))\n",
    "    for filename in range(os_list_range):\n",
    "        img = Image.open(folder + \"/\" + str(filename) + '.png')\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c86dcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "from PIL import Image \n",
    "import PIL \n",
    "\n",
    "def downsample(): \n",
    "    for directory in Directories:\n",
    "        images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR/Original')\n",
    "        for scale in scales: \n",
    "            disk_cropped = '/Users/ege/Desktop/DataSets/' + directory + '/HR/Original_Cropped/'\n",
    "            disk_dir = '/Users/ege/Desktop/DataSets/' + directory + \"/LR/\" + str(scale) + \"_b/\"\n",
    "            print(scale)\n",
    "            for i, oriimg in enumerate(images): \n",
    "                width, height = oriimg.size\n",
    "                \n",
    "                Mod_1 = oriimg.size[0]%8\n",
    "                Mod_2 = oriimg.size[1]%8\n",
    "                right = width\n",
    "                bottom = height\n",
    "\n",
    "                cropped_img_1 = oriimg.crop((0, 0, right-Mod_1, bottom-Mod_2))\n",
    "                \n",
    "                width, height = cropped_img_1.size\n",
    "                \n",
    "                img = cropped_img_1.resize((int(width/scale),int(height/scale)), Image.BICUBIC)\n",
    "                              \n",
    "                cropped_img_1.save(disk_cropped +str(i)+ \".png\",\"PNG\")\n",
    "                \n",
    "                img.save(disk_dir +str(i)+ \".png\",\"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73836abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sewar.full_ref import mse, rmse, psnr, rmse_sw, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb \n",
    "\n",
    "def evaluate(img1,img2):\n",
    "    print(ssim(img1,img2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b627682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample\n",
    "# Load Images \n",
    "\n",
    "Directories = ['Set5', 'Set14', 'Urban100', 'BSD100', 'Celeba_HQ']\n",
    "scales = [2,4,8]\n",
    "\n",
    "def upsample():\n",
    "    for directory in Directories:\n",
    "        for scale in scales: \n",
    "            images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/LR/\" + str(scale) + '_b')\n",
    "            disk_dir = '/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\"\n",
    "            for i, oriimg in enumerate(images): \n",
    "                width, height = oriimg.size\n",
    "                img = oriimg.resize((int(width*scale),int(height*scale)), Image.BICUBIC)\n",
    "                #img = cv2.resize(oriimg,None, fx=scale, fy=scale, interpolation = cv2.INTER_CUBIC)\n",
    "                img.save(disk_dir +str(i)+ \".png\",\"PNG\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71c8939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "import math\n",
    "import cv2\n",
    "\n",
    "def psnr_numpy(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5b047956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        0      0  0   0           0          0          0  \\\n",
      "0   bicubic  bicubic  Set14  4   0  340.186376  18.444142  22.813634   \n",
      "1   bicubic  bicubic  Set14  4   1  587.927949  24.247226  20.437563   \n",
      "2   bicubic  bicubic  Set14  4   2  613.605899  24.771070  20.251908   \n",
      "3   bicubic  bicubic  Set14  4   3  285.003467  16.882046  23.582302   \n",
      "4   bicubic  bicubic  Set14  4   4  235.425629  15.343586  24.412266   \n",
      "5   bicubic  bicubic  Set14  4   5  154.345134  12.423572  26.245874   \n",
      "6   bicubic  bicubic  Set14  4   6  321.561325  17.932131  23.058166   \n",
      "7   bicubic  bicubic  Set14  4   7  259.378577  16.105234  23.991463   \n",
      "8   bicubic  bicubic  Set14  4   8   95.067094   9.750236  28.350501   \n",
      "9   bicubic  bicubic  Set14  4   9  166.562507  12.905910  25.915031   \n",
      "10  bicubic  bicubic  Set14  4  10   83.656570   9.146397  28.905803   \n",
      "11  bicubic  bicubic  Set14  4  11  610.575448  24.709825  20.273410   \n",
      "12  bicubic  bicubic  Set14  4  12  270.753818  16.454599  23.805058   \n",
      "13  bicubic  bicubic  Set14  4  13  124.454281  11.155908  27.180705   \n",
      "\n",
      "            0         0                   0         0  \n",
      "0   22.779119  0.150611  0.909441+0.000000j  0.150611  \n",
      "1   19.848709  0.116170  0.947340+0.000000j  0.116170  \n",
      "2   20.489869  0.162641  0.893238+0.000000j  0.162641  \n",
      "3   23.630777  0.141863  0.910320+0.000000j  0.141863  \n",
      "4   24.394289  0.145022  0.922387+0.000000j  0.145022  \n",
      "5   26.539516  0.115668  0.969623+0.000000j  0.115668  \n",
      "6   23.032723  0.142445  0.888437+0.000000j  0.142445  \n",
      "7   24.009192  0.124143  0.835552+0.000000j  0.124143  \n",
      "8   28.503951  0.078977  0.951180+0.000000j  0.078977  \n",
      "9   26.168310  0.073410  0.970256+0.000000j  0.073410  \n",
      "10  29.059609  0.093810  0.944227+0.000000j  0.093810  \n",
      "11  19.979935  0.212937  0.839863+0.000000j  0.212937  \n",
      "12  24.320624  0.156410  0.928231+0.000000j  0.156410  \n",
      "13  28.194903  0.094729  0.964371+0.000000j  0.094729  \n"
     ]
    }
   ],
   "source": [
    "# Evaluation using library and metric using rgb \n",
    "\n",
    "Directories = ['Set14']\n",
    "scales = [4]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sewar.full_ref import mse, rmse, rmse_sw, uqi, ssim, ergas, scc, rase, sam, msssim, vifp, psnrb, psnr\n",
    "\n",
    "metric_name = []\n",
    "upsampling_method = []\n",
    "downsampling_method = []\n",
    "image_set = []\n",
    "image_scale = [] \n",
    "image_number = []\n",
    "metric_1 = []\n",
    "metric_2 = []\n",
    "metric_3 = []\n",
    "metric_4 = []\n",
    "metric_5 = []\n",
    "metric_6 = []\n",
    "metric_7 = []\n",
    "metric_8 = []\n",
    "\n",
    "for directory in Directories:    \n",
    "    for scale in scales: \n",
    "        images_1 = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + \"Original_Cropped\") \n",
    "        images_2 =  load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\")\n",
    "\n",
    "        #images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR')\n",
    "        for i, oriimg in enumerate(images_1): \n",
    "            #evaluated_value = ssim(oriimg,images_2[i]) #evaluate(oriimg,images_2[i])\n",
    "            #value_mse = uqi(images_1[i],images_2[i])\n",
    "            #print(value_mse)\n",
    "            \n",
    "            pil_image_1 = images_1[i].convert('RGB') \n",
    "            pil_image_2 = images_2[i].convert('RGB')\n",
    "            \n",
    "            open_cv_image_1 = np.array(pil_image_1) \n",
    "            open_cv_image_2 = np.array(pil_image_2) \n",
    "            \n",
    "            # Convert RGB to BGR \n",
    "            open_cv_image_1 = open_cv_image_1[:, :, ::-1].copy()\n",
    "            open_cv_image_2 = open_cv_image_2[:, :, ::-1].copy()\n",
    "            \n",
    "            try:\n",
    "                mse_value = mse(open_cv_image_1,open_cv_image_2) #evaluate(oriimg,images_2[i])\n",
    "                rmse_value = rmse(open_cv_image_1,open_cv_image_2)\n",
    "                psnr_value = psnr(open_cv_image_1,open_cv_image_2)\n",
    "                rmse_sw_value = psnr_numpy(open_cv_image_1,open_cv_image_2)\n",
    "                ssim_value = ssim(open_cv_image_1,open_cv_image_2)\n",
    "                sam_value = sam(open_cv_image_1,open_cv_image_2)\n",
    "                msssim_value = msssim(open_cv_image_1,open_cv_image_2)\n",
    "                psnrb_value = psnrb(open_cv_image_1,open_cv_image_2) \n",
    "                \n",
    "            except:\n",
    "                mse_value = None\n",
    "                rmse_value = None\n",
    "                psnr_value = None\n",
    "                ssim_value = None\n",
    "                sam_value = None\n",
    "                msssim_value = None\n",
    "                psnrb_value = None\n",
    "                \n",
    "                print('Folder:' + str(scale) + '_upscaled image number:' + str(i))\n",
    "                pass\n",
    "            \n",
    "\n",
    "            # Merge all the lists/Pandas Dataframes in one and then concatenate them as a LIST\n",
    "            upsampling_method.append('bicubic')\n",
    "            downsampling_method.append('bicubic')\n",
    "            image_set.append(str(directory))\n",
    "            image_scale.append(str(scale))\n",
    "            image_number.append(str(i))\n",
    "            metric_1.append(mse_value)\n",
    "            metric_2.append(rmse_value)\n",
    "            metric_3.append(psnr_value)\n",
    "            metric_5.append(psnrb_value)\n",
    "            metric_6.append(sam_value)\n",
    "            metric_7.append(msssim_value)\n",
    "            metric_8.append(sam_value)\n",
    "            \n",
    "\n",
    "            \n",
    "d_1 =  pd.DataFrame(metric_1)\n",
    "d_2 =  pd.DataFrame(metric_2)\n",
    "d_3 =  pd.DataFrame(metric_3)\n",
    "d_5 =  pd.DataFrame(metric_5)\n",
    "d_6 =  pd.DataFrame(metric_6)\n",
    "d_7 =  pd.DataFrame(metric_7)\n",
    "d_8 =  pd.DataFrame(metric_8)\n",
    "m_1 =  pd.DataFrame(upsampling_method)\n",
    "m_2 =  pd.DataFrame(downsampling_method)\n",
    "i_s =  pd.DataFrame(image_set)\n",
    "i_m =  pd.DataFrame(image_scale)\n",
    "i_n =  pd.DataFrame(image_number)\n",
    "\n",
    "\n",
    "        \n",
    "frames = [m_1,m_2,i_s,i_m,i_n,d_1,d_2,d_3,d_5,d_6,d_7,d_8]\n",
    "result = pd.concat(frames, axis=1)\n",
    "\n",
    "            # Save the big LIST again as Dataframe and then save it as list_2.csv\n",
    "df = pd.DataFrame(result)\n",
    "print(df)\n",
    "df.to_csv('first_evaluation_test_2.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d263f2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    24.230263\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(d_3.mean(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4f13b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookifaxisaresame(img1,img2):\n",
    "    \n",
    "    crp_img2 = img2.crop((0, 0, img1.size[0], img1.size[1]))\n",
    "\n",
    "    return img1, crp_img2\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "36f33f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0        0      0  0   0          0         0\n",
      "0   bicubic  bicubic  Set14  2   0  30.666347  0.909632\n",
      "1   bicubic  bicubic  Set14  2   1  26.585917  0.946814\n",
      "2   bicubic  bicubic  Set14  2   2  26.005674  0.852417\n",
      "3   bicubic  bicubic  Set14  2   3  27.942775  0.840608\n",
      "4   bicubic  bicubic  Set14  2   4  29.265910  0.845599\n",
      "5   bicubic  bicubic  Set14  2   5  32.931336  0.958816\n",
      "6   bicubic  bicubic  Set14  2   6  27.815216  0.803186\n",
      "7   bicubic  bicubic  Set14  2   7  29.133704  0.789878\n",
      "8   bicubic  bicubic  Set14  2   8  34.145069  0.895031\n",
      "9   bicubic  bicubic  Set14  2   9  32.717302  0.950129\n",
      "10  bicubic  bicubic  Set14  2  10  35.004025  0.863210\n",
      "11  bicubic  bicubic  Set14  2  11  24.530176  0.696286\n",
      "12  bicubic  bicubic  Set14  2  12  30.523492  0.899284\n",
      "13  bicubic  bicubic  Set14  2  13  33.257687  0.912279\n",
      "          0        0       0  0   0          0         0\n",
      "0   bicubic  bicubic  BSD100  2   0  32.314055  0.899080\n",
      "1   bicubic  bicubic  BSD100  2   1  29.796060  0.928398\n",
      "2   bicubic  bicubic  BSD100  2   2  28.050072  0.790567\n",
      "3   bicubic  bicubic  BSD100  2   3  23.181740  0.615173\n",
      "4   bicubic  bicubic  BSD100  2   4  34.067370  0.899984\n",
      "..      ...      ...     ... ..  ..        ...       ...\n",
      "95  bicubic  bicubic  BSD100  2  95  31.006065  0.878861\n",
      "96  bicubic  bicubic  BSD100  2  96  28.304783  0.811715\n",
      "97  bicubic  bicubic  BSD100  2  97  34.709624  0.942206\n",
      "98  bicubic  bicubic  BSD100  2  98  28.811232  0.927300\n",
      "99  bicubic  bicubic  BSD100  2  99  27.648693  0.819953\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "         0        0     0  0  0          0         0\n",
      "0  bicubic  bicubic  Set5  2  0  37.119882  0.950083\n",
      "1  bicubic  bicubic  Set5  2  1  32.092753  0.947770\n",
      "2  bicubic  bicubic  Set5  2  2  36.861960  0.971170\n",
      "3  bicubic  bicubic  Set5  2  3  27.424867  0.908069\n",
      "4  bicubic  bicubic  Set5  2  4  35.027003  0.861848\n",
      "          0        0         0  0   0          0         0\n",
      "0   bicubic  bicubic  Urban100  2   0  35.542851  0.956899\n",
      "1   bicubic  bicubic  Urban100  2   1  27.929934  0.839134\n",
      "2   bicubic  bicubic  Urban100  2   2  19.953670  0.814514\n",
      "3   bicubic  bicubic  Urban100  2   3  28.879806  0.898918\n",
      "4   bicubic  bicubic  Urban100  2   4  27.796067  0.850643\n",
      "..      ...      ...       ... ..  ..        ...       ...\n",
      "95  bicubic  bicubic  Urban100  2  95  27.075162  0.819367\n",
      "96  bicubic  bicubic  Urban100  2  96  26.076883  0.746528\n",
      "97  bicubic  bicubic  Urban100  2  97  29.208366  0.828534\n",
      "98  bicubic  bicubic  Urban100  2  98  23.826879  0.829729\n",
      "99  bicubic  bicubic  Urban100  2  99  21.396996  0.719619\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "          0        0          0  0   0          0         0\n",
      "0   bicubic  bicubic  Celeba_HQ  2   0  48.048917  0.990312\n",
      "1   bicubic  bicubic  Celeba_HQ  2   1  44.311688  0.988459\n",
      "2   bicubic  bicubic  Celeba_HQ  2   2  47.644574  0.988603\n",
      "3   bicubic  bicubic  Celeba_HQ  2   3  47.030544  0.994387\n",
      "4   bicubic  bicubic  Celeba_HQ  2   4  45.828012  0.990491\n",
      "..      ...      ...        ... ..  ..        ...       ...\n",
      "95  bicubic  bicubic  Celeba_HQ  2  95  49.232361  0.994731\n",
      "96  bicubic  bicubic  Celeba_HQ  2  96  39.845527  0.980390\n",
      "97  bicubic  bicubic  Celeba_HQ  2  97  42.570523  0.989103\n",
      "98  bicubic  bicubic  Celeba_HQ  2  98  37.878969  0.961027\n",
      "99  bicubic  bicubic  Celeba_HQ  2  99  43.443410  0.980171\n",
      "\n",
      "[100 rows x 7 columns]\n",
      "           0          0         0  0\n",
      "0      Set14  30.037473  0.868798  2\n",
      "1     BSD100  29.566708  0.840796  2\n",
      "2       Set5  33.705293  0.927788  2\n",
      "3   Urban100  26.587055  0.837458  2\n",
      "4  Celeba_HQ  44.543068  0.984939  2\n"
     ]
    }
   ],
   "source": [
    "#Evaluating by hardcoded psnr and sssim and metrics YCBCR information\n",
    "Directories = ['Set14','BSD100','Set5','Urban100','Celeba_HQ']\n",
    "scales = [2]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2ycbcr\n",
    "from skimage import data, img_as_float\n",
    "\n",
    "\n",
    "from sewar.full_ref import mse, rmse, rmse_sw, uqi, ergas, scc, rase, sam, msssim, vifp, psnrb, psnr\n",
    "\n",
    "Image_DataSet_List = []\n",
    "Image_Scaled_List = []\n",
    "Average_Metric_0_List = []\n",
    "Average_Metric_1_List = []\n",
    "Scale_List = []\n",
    "\n",
    "\n",
    "for directory in Directories:  \n",
    "    metric_name = []\n",
    "    upsampling_method = []\n",
    "    downsampling_method = []\n",
    "    image_set = []\n",
    "    image_scale = [] \n",
    "    image_number = []\n",
    "    metric_1 = []\n",
    "    metric_2 = []\n",
    "    for scale in scales: \n",
    "        images_1 = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + \"Original_Cropped\") \n",
    "        images_2 =  load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + \"/HR/\" + str(scale) + \"_upscaled/\")\n",
    "        Scale_List.append(scale)   \n",
    "        #images = load_images_from_folder('/Users/ege/Desktop/DataSets/' + directory + '/HR')\n",
    "        for i, oriimg in enumerate(images_1): \n",
    "            \n",
    "            \n",
    "            if (images_1[i].size != images_2[i].size):\n",
    "                images_1[i], images_2[i] = lookifaxisaresame(images_1[i],images_2[i])\n",
    "            \n",
    "            pil_image_1 = images_1[i].convert('RGB') \n",
    "            pil_image_2 = images_2[i].convert('RGB')\n",
    "            \n",
    "            open_cv_image_1 = np.array(pil_image_1) \n",
    "            open_cv_image_2 = np.array(pil_image_2) \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            # Convert RGB to BGR \n",
    "            open_cv_image_1 = open_cv_image_1[:, :, ::-1].copy()\n",
    "            open_cv_image_2 = open_cv_image_2[:, :, ::-1].copy()\n",
    "            \n",
    "            im1_t = np.atleast_3d(img_as_float(open_cv_image_1))\n",
    "            im2_t = np.atleast_3d(img_as_float(open_cv_image_2))\n",
    "\n",
    "            if im1_t.shape[2] == 1 or im2_t.shape[2] == 1:\n",
    "                im1_t = im1_t[..., 0]\n",
    "                im2_t = im2_t[..., 0]\n",
    "\n",
    "            else:\n",
    "                im1_t = rgb2ycbcr(im1_t)[:, :, 0:1] / 255.0\n",
    "                im2_t = rgb2ycbcr(im2_t)[:, :, 0:1] / 255.0\n",
    "\n",
    "            try: \n",
    "                psnr_val = peak_signal_noise_ratio(im1_t, im2_t)\n",
    "                structural_similarity_val = ssim(im1_t,im2_t,win_size=11,gaussian_weights=True,multichannel=True,data_range=1.0,K1=0.01,K2=0.03,sigma=1.5)                #print(structural_similarity(open_cv_image_1, open_cv_image_2))\n",
    "\n",
    "\n",
    "            except:\n",
    "                print('error at: ' + str(i))\n",
    "                psnr_val = None\n",
    "                structural_similarity_val = None\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "            # Merge all the lists/Pandas Dataframes in one and then concatenate them as a LIST\n",
    "            upsampling_method.append('bicubic')\n",
    "            downsampling_method.append('bicubic')\n",
    "            image_set.append(str(directory))\n",
    "            image_scale.append(str(scale))\n",
    "            image_number.append(str(i))\n",
    "            metric_1.append(psnr_val)\n",
    "            \n",
    "            metric_2.append(structural_similarity_val)\n",
    "          \n",
    "        \n",
    "        \n",
    "    d_1 =  pd.DataFrame(metric_1)\n",
    "    d_2 =  pd.DataFrame(metric_2)\n",
    "    m_1 =  pd.DataFrame(upsampling_method)\n",
    "    m_2 =  pd.DataFrame(downsampling_method)\n",
    "    i_s =  pd.DataFrame(image_set)\n",
    "    i_m =  pd.DataFrame(image_scale)\n",
    "    i_n =  pd.DataFrame(image_number)\n",
    "\n",
    "    frames = [m_1,m_2,i_s,i_m,i_n,d_1,d_2]\n",
    "    result = pd.concat(frames, axis=1)\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    print(df)\n",
    "    \n",
    "    Image_DataSet_List.append(directory)\n",
    "    Average_Metric_0_List.append(d_1.mean())\n",
    "    Average_Metric_1_List.append(d_2.mean())\n",
    "   \n",
    "    \n",
    "\n",
    "I_D_L =  pd.DataFrame(Image_DataSet_List)\n",
    "A_M_0 =  pd.DataFrame(Average_Metric_0_List)\n",
    "A_M_1 =  pd.DataFrame(Average_Metric_1_List)\n",
    "S_L   =  pd.DataFrame(Scale_List)\n",
    "    \n",
    "frames_overall = [I_D_L,A_M_0,A_M_1,S_L]\n",
    "result_overall = pd.concat(frames_overall, axis=1)\n",
    "df_overall= pd.DataFrame(result_overall)\n",
    "\n",
    "print(df_overall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846088dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e62c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
